{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    \n   \n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" !nvidia-smi ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch import optim\nimport torch.nn.functional as F\nimport csv\nimport random\nimport re\nimport os\nimport unicodedata\nimport codecs\nimport itertools\nimport math\n\nUSE_CUDA = torch.cuda.is_available()\ndevice = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.cuda.get_device_name(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> # **IMPORTING LIBARARIES**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":" > # **Load & Preprocess Data**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lines_filepath = '/kaggle/input/chatbot-dataset/movie_lines.txt'\nconv_filepath = '/kaggle/input/chatbot-dataset/movie_conversations.txt'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#visualize some movie lines...\ncorpus_name = \"cornell movie-dialogs corpus\"\nwith open(lines_filepath,'r',encoding='iso-8859-1') as file:\n    lines = file.readlines()\nfor line in lines[30:50]:\n    print(line.strip())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split each line of the file into Dictionary of fields(lineID,characterID,movieID,character,text)\nline_fields = [\"lineID\",\"characterID\",\"movieID\",\"character\",\"text\"]\nlines = {}\nwith open(lines_filepath, 'r', encoding='iso-8859-1') as f:\n    for line in f:\n        values = line.split(' +++$+++ ')\n        #Extract Fields\n        lineObj = {}\n        for i,field in enumerate(line_fields):\n            lineObj[field] = values[i]\n        lines[lineObj['lineID']] = lineObj    \n            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lines['L1042'],lines['L1043']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# groups fields of lines from 'Loadlines' into conversations based on \"movie_conversation.txt\"\nconv_fields = [\"character1ID\",\"character2ID\",\"movieID\",\"utteranceIDs\"]\nconversations = []\nwith open(conv_filepath, 'r', encoding='iso-8859-1') as f:\n    for line in f:\n        values = line.split(\" +++$+++ \")\n        #Extract fields\n        convObj = {}\n       \n        for i,field in enumerate(conv_fields):\n            convObj[field] = values[i]\n        #convert string results from split to list. convObj['utteranceIDs'] == ['L1','L2',...]\n            \n        lineIds = eval(convObj[\"utteranceIDs\"])\n        #reassemble lines\n        convObj['lines'] = []\n        for lineId in lineIds:\n            convObj['lines'].append(lines[lineId]) \n        conversations.append(convObj)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"conversations[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#extract pair of sentences from conversations\nqa_pairs = []\nfor conversation in conversations:\n    for i in range(len(conversation['lines'])-1):\n        imputline = conversation['lines'][i]['text'].strip()\n        targetline = conversation['lines'][i+1]['text'].strip()\n    \n        if imputline and targetline:\n            qa_pairs.append([imputline,targetline])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(qa_pairs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"qa_pairs[2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#define path to new file\noutdirname= '/kaggle/working'\ndatafile = os.path.join(outdirname,\"formatted_movie_lines.txt\")\ndelimiter = '\\t'\ndelimiter = str(codecs.decode(delimiter,'unicode_escape'))\n\n#write new csv file\nprint(\"\\nwriting newly formatted file...\")\nwith open(datafile,'w',encoding = 'utf-8') as outputfile:\n    writer = csv.writer(outputfile,delimiter=delimiter)\n    for pair in qa_pairs:\n        writer.writerow(pair)\nprint(\"Done writing to file...\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#visualize some lines\ndatafile = os.path.join(outdirname,\"formatted_movie_lines.txt\")\nwith open(datafile,'rb') as file:\n    lines = file.readlines()\nfor line in lines[:8]:\n    print(line)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# making the vocabulary\nPAD_token = 0  #for padding short senetnces\nSOS_token = 1  #start of sentence token<START>\nEOS_token = 2  #end of sentence token<END>\n\nclass vocabulary:\n    def __init__(self,name):\n        self.name = name\n        self.word2index = {}\n        self.word2count = {}\n        self.index2word = {PAD_token: 'PAD',SOS_token:'SOS',EOS_token:'EOS'}\n        self.num_words = 3 # count PAD,SOS,EOS\n        \n    def addSentence(self,sentence):\n        for word in sentence.split(' '):\n            self.addWord(word)\n    \n    def addWord(self,word):\n        if word not in self.word2index:\n            self.word2index[word] = self.num_words\n            self.word2count[word] = 1\n            self.index2word[self.num_words] = word\n            self.num_words +=1\n        else:\n            self.word2count[word] += 1\n    \n    #remove words below a threshhold\n    def trim(self, min_count):\n        keep_words = []\n        for k,v in self.word2count.items():\n            if v >= min_count:\n                keep_words.append(k)\n        \n        print('keep_words {} / {} = {:.4f}'.format(\n            len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)\n        ))\n        \n        # Reinitialize dictionaries\n        self.word2index = {}\n        self.word2count = {}\n        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n        self.num_words = 3 # Count default tokens\n\n        for word in keep_words:\n            self.addWord(word)\n            \n        \n            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#turn a unicode string to plain ASCII\ndef unicodeToAscii(s):\n    return ''.join(c for c in unicodedata.normalize('NFD',s) if unicodedata.category(c) != 'Mn')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test the function\nunicodeToAscii(\"á Á é É í Í ó\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lowercase, trim, and remove non-letter characters\ndef normalizeString(s):\n    s = unicodeToAscii(s.lower().strip())\n    #Replace any .!? by a whitespace + that character ('!' by ' !'). \\1 means the first bracketed group[.!?].\n    # r is to not consider \\1 as escape character.\n    \n    #s= re.sub(r\"([.+])\", r\".\", s)\n    #s= re.sub(\"\\,+\",\",\",s)\n    s= re.sub(r\"\\.+\",r\" \",s)\n    s= re.sub(r\"\\?+\",r\"?\",s)\n    s= re.sub(r\"\\!+\",r\"!\",s)\n    s=s+'.'\n    s=re.sub(r\"\\?.\",r\"?\",s)\n    s=re.sub(r\"\\!.\",r\"!\",s)\n    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n    # Remove any character that is nota sequence of lower and upper case letters. + means one or more.\n    s = re.sub(r\"[^a-zA-Z.'!?]+\", r\" \", s)\n        \n    #remove sequence of whitespace characters.\n    s = re.sub(r\"\\s+\", r\" \", s).strip()\n    return s","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test the function\nnormalizeString(\"he's a reader--..!\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"datafile = os.path.join(outdirname, \"formatted_movie_lines.txt\")\n# readthe file and split into lines\nprint(\"Readaing and processing file... please wait.\")\nlines = open(datafile, encoding = 'utf-8').read().strip().split('\\n')\n#split every line into pairs and normalize.\npairs =[[normalizeString(s) for s in pair.split(\"\\t\")] for pair in lines]\nprint(\"Done reading...\")\nvoc = vocabulary(\"cornell movie-dialogs corpus\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pairs[4]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Returns True if both the sentences in a pair 'p' are under the MAX_LENGTH Threshhold.\nMAX_LENGTH = 10\ndef filterPair(p):\n    #input sequences must preserve the last word for EOS token\n    return len(p[0].split()) < MAX_LENGTH and len(p[1].split()) < MAX_LENGTH\n#Filter pairs using filter pair conditions.\ndef filterPairs(pairs):\n    return [pair for pair in pairs if filterPair(pair)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pairs =[pair for pair in pairs if len(pair)>1]\nprint(\"There are {} pairs/conversations\".format(len(pairs)))\npairs = filterPairs(pairs)\nprint(\"There are {} pairs/conversations\".format(len(pairs)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#loop through each pair and add question and reply sequence to vocabulary\nfor pair in pairs:\n    voc.addSentence(pair[0])\n    voc.addSentence(pair[1])\nprint(\"Number of words:\",voc.num_words)\nfor pair in pairs[10:20]:\n    print(pair)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MIN_COUNT = 3    # Minimum word count threshold for trimming\n\ndef trimRareWords(voc, pairs, MIN_COUNT):\n    # Trim words used under the MIN_COUNT from the voc\n    voc.trim(MIN_COUNT)\n    # Filter out pairs with trimmed words\n    keep_pairs = []\n    for pair in pairs:\n        input_sentence = pair[0]\n        output_sentence = pair[1]\n        keep_input = True\n        keep_output = True\n        # Check input sentence\n        for word in input_sentence.split(' '):\n            if word not in voc.word2index:\n                keep_input = False\n                break\n        # Check output sentence\n        for word in output_sentence.split(' '):\n            if word not in voc.word2index:\n                keep_output = False\n                break\n\n        # Only keep pairs that do not contain trimmed word(s) in their input or output sentence\n        if keep_input and keep_output:\n            keep_pairs.append(pair)\n\n    print(\"Trimmed from {} pairs to {}, {:.4f} of total\".format(len(pairs), len(keep_pairs), len(keep_pairs) / len(pairs)))\n    return keep_pairs\n\n\n\n\n# Trim voc and pairs\npairs = trimRareWords(voc, pairs, MIN_COUNT)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def indexesFromSentence(voc, sentence):\n    return [voc.word2index[word] for word in sentence.split(' ')] + [EOS_token]\n\n# It zips each entries with corresponding entries in other list and pads 0 where there is no entry,\n# in this process the original matrix is transposed automatically. \ndef zeroPadding(l, fillvalue=PAD_token): #pads the matrix with fillvalue(which is 0 here) where it is short, to make it m x n.\n    return list(itertools.zip_longest(*l, fillvalue=fillvalue)) #where m is length of longest question which is 10 here.\n\ndef binaryMatrix(l, value=PAD_token): # creates binary matrix, if there is a value, 1 is stored else 0 for no value.\n    m = []\n    for i, seq in enumerate(l):\n        m.append([])                #for every list of values there is a list \n        for token in seq:\n            if token == PAD_token:\n                m[i].append(0)\n            else:\n                m[i].append(1)\n    return m\n\n# Returns padded input sequence tensor and lengths\ndef inputVar(l, voc):\n    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n    padList = zeroPadding(indexes_batch)\n    padVar = torch.LongTensor(padList)\n    return padVar, lengths\n\n# Returns padded target sequence tensor, padding mask, and max target length\ndef outputVar(l, voc):\n    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n    max_target_len = max([len(indexes) for indexes in indexes_batch])\n    padList = zeroPadding(indexes_batch)\n    mask = binaryMatrix(padList)\n    mask = torch.BoolTensor(mask)\n    padVar = torch.LongTensor(padList)\n    return padVar, mask, max_target_len\n\n# Returns all items for a given batch of pairs\n#1 sorting batch with key as length means the qa_pair with longer question length is before all other pair in the list.\ndef batch2TrainData(voc, pair_batch):\n    pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True) #1\n    input_batch, output_batch = [], []\n    for pair in pair_batch:\n        input_batch.append(pair[0])\n        output_batch.append(pair[1])\n    inp, lengths = inputVar(input_batch, voc)\n    output, mask, max_target_len = outputVar(output_batch, voc)\n    return inp, lengths, output, mask, max_target_len\n\n\n# Example for validation\nsmall_batch_size = 5\nbatches = batch2TrainData(voc, [random.choice(pairs) for _ in range(small_batch_size)])\ninput_variable, lengths, target_variable, mask, max_target_len = batches\n\nprint(\"input_variable:\", input_variable) # input transposed when matrix was padded above.\nprint(\"lengths:\", lengths) # in desc order bcoz of sorting with key as length of question\nprint(\"target_variable:\", target_variable) # answer corresponds to each question(column) in the input matrix\nprint(\"mask:\", mask)\nprint(\"max_target_len:\", max_target_len)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ## Seq2Seq Model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":">  ### Encoder ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Inputs:\n\n    input_seq: batch of input sentences; shape=(max_length, batch_size)...\n   \n    input_lengths: list of sentence lengths corresponding to each sentence in the batch; shape=(batch_size)...\n   \n    hidden: hidden state; shape=(n_layers x num_directions, batch_size, hidden_size)\n\nOutputs:\n\n    outputs: output features from the last hidden layer of the GRU (sum of bidirectional outputs); shape=(max_length, batch_size, hidden_size)...\n   \n    hidden: updated hidden state from GRU; shape=(n_layers x num_directions, batch_size, hidden_size)\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class EncoderRNN(nn.Module):\n    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n        super(EncoderRNN, self).__init__()\n        self.n_layers = n_layers\n        self.hidden_size = hidden_size\n        self.embedding = embedding\n\n        # Initialize GRU; the input_size and hidden_size params are both set to 'hidden_size'\n        #   because our input size is a word embedding with number of features == hidden_size\n        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,\n                          dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\n\n    def forward(self, input_seq, input_lengths, hidden=None):\n        # Convert word indexes to embeddings\n        embedded = self.embedding(input_seq)\n        # Pack padded batch of sequences for RNN module\n        packed = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n        # Forward pass through GRU\n        outputs, hidden = self.gru(packed, hidden)\n        # Unpack padding\n        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n        # Sum bidirectional GRU outputs\n        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n        # Return output and final hidden state\n        return outputs, hidden","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### Decoder","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"> attention model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Luong attention layer\nclass Attn(nn.Module):\n    def __init__(self, method, hidden_size):\n        super(Attn, self).__init__()\n        self.method = method\n        if self.method not in ['dot', 'general', 'concat']:\n            raise ValueError(self.method, \"is not an appropriate attention method.\")\n        self.hidden_size = hidden_size\n        if self.method == 'general':\n            self.attn = nn.Linear(self.hidden_size, hidden_size)\n        elif self.method == 'concat':\n            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n            self.v = nn.Parameter(torch.FloatTensor(hidden_size))\n\n    def dot_score(self, hidden, encoder_output):\n        #Element wise multiply the current decoder state with the encoder output and sum them along dim=2.\n        return torch.sum(hidden * encoder_output, dim=2)\n\n    def general_score(self, hidden, encoder_output):\n        energy = self.attn(encoder_output)\n        return torch.sum(hidden * energy, dim=2)\n\n    def concat_score(self, hidden, encoder_output):\n        energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()\n        return torch.sum(self.v * energy, dim=2)\n\n    def forward(self, hidden, encoder_outputs):\n        # hidden of shape: (1, batch_size, hidden_size)\n        #encoder_outputs of shape: (max_length, batch_size, hidden_size)\n        \n        # Calculate the attention weights (energies) based on the given method\n        if self.method == 'general':\n            attn_energies = self.general_score(hidden, encoder_outputs)\n        elif self.method == 'concat':\n            attn_energies = self.concat_score(hidden, encoder_outputs)\n        elif self.method == 'dot':\n            attn_energies = self.dot_score(hidden, encoder_outputs) # (max_length,batch_size),after summing along hiddden size(dim=2)\n\n        # Transpose max_length and batch_size dimensions\n        attn_energies = attn_energies.t()                           # (batch_size, max_length)\n\n        # Return the softmax normalized probability scores (with added dimension)\n        return F.softmax(attn_energies, dim=1).unsqueeze(1)    # (batch_size, 1, max_length)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Luong Attention Decoder","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Inputs:\n\n    input_step: one time step (one word) of input sequence batch; shape=(1, batch_size)\n    last_hidden: final hidden layer of GRU; shape=(n_layers x num_directions, batch_size, hidden_size)\n    encoder_outputs: encoder model’s output; shape=(max_length, batch_size, hidden_size)\n\nOutputs:\n\n    output: softmax normalized tensor giving probabilities of each word being the correct next word in the decoded sequence; shape=(batch_size, voc.num_words)\n    hidden: final hidden state of GRU; shape=(n_layers x num_directions, batch_size, hidden_size)\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class LuongAttnDecoderRNN(nn.Module):\n    def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n        super(LuongAttnDecoderRNN, self).__init__()\n\n        # Keep for reference\n        self.attn_model = attn_model\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.n_layers = n_layers\n        self.dropout = dropout\n\n        # Define layers\n        self.embedding = embedding\n        self.embedding_dropout = nn.Dropout(dropout)\n        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n        self.out = nn.Linear(hidden_size, output_size)\n\n        self.attn = Attn(attn_model, hidden_size)\n\n    def forward(self, input_step, last_hidden, encoder_outputs):\n        #input_step: one time step (1 word) of input sequence batch, shape = (1,batch_size)\n        #last_hidden: final hidden state of encoder's GRU; shape = (n_layers x num_direcions, batch_size, hidden_size)\n        #encoder_outputs: encoder model's output; shape = (max_length, batch_size, hidden_size)\n        \n        # Note: we run this one step (word) at a time\n        # Get embedding of current input word\n        embedded = self.embedding(input_step)\n        embedded = self.embedding_dropout(embedded)\n        # Forward through unidirectional GRU\n        rnn_output, hidden = self.gru(embedded, last_hidden)\n        #rnn_output of shape: (1,batch,num_directions x hidden_size)\n        #last_hidden of shape: (n_layers x num_direcions, batch_size, hidden_size)\n        \n        # Calculate attention weights from the current GRU output\n        attn_weights = self.attn(rnn_output, encoder_outputs)\n        # Multiply attention weights to encoder outputs to get new \"weighted sum\" context vector\n        #(batch_size, 1, max_length) bmm with (batch_size, max_length, hidden) = (batch_size, 1, hidden)\n        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n        # Concatenate weighted context vector and GRU output using Luong eq. 5\n        rnn_output = rnn_output.squeeze(0)\n        context = context.squeeze(1)\n        concat_input = torch.cat((rnn_output, context), 1)\n        concat_output = torch.tanh(self.concat(concat_input))\n        # Predict next word using Luong eq. 6\n        output = self.out(concat_output)\n        output = F.softmax(output, dim=1)\n        # Return output and final hidden state\n        return output, hidden","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ## Traning the model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"> Masked LOSS","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#This loss function calculates the average negative log likelihood of the elements\n#that correspond to a 1 in the mask tensor.\n\ndef maskNLLLoss(inp, target, mask):\n    nTotal = mask.sum()\n    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n    loss = crossEntropy.masked_select(mask).mean()\n    loss = loss.to(device)\n    return loss, nTotal.item()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#visualisation of loss function\n#decoder_output shape:(batcH_size,vocab_size); target size = (batch_size,1)\ndec_o = torch.rand(5,7)\nprint(dec_o)\ndec_o = F.softmax(dec_o,dim = 1)\ntar = torch.tensor([2,1,5,4,0],dtype = torch.long)\ntar = tar.view(-1,1)\nmask = torch.tensor([1,0,1,1,0],dtype= torch.uint8)\nprint(dec_o)\nprint(tar)\ngath_ten = torch.gather(dec_o,1,tar)\nprint(gath_ten)\nprint(gath_ten.shape)\ncrossEntropy = -torch.log(gath_ten)\nprint(\"Cross Entropy:\", crossEntropy)\nmask = mask.unsqueeze(1)\nloss = crossEntropy.masked_select(mask)\nprint(\"Loss :\",loss)\nprint(loss.shape)\nprint(\"sum of the mask element: \",mask.sum())\nprint(\"Mean of loss : \", loss.mean())\nprint(\"Mean of cross entropy without mask...: \",crossEntropy.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding,\n          encoder_optimizer, decoder_optimizer, batch_size, clip, max_length=MAX_LENGTH):\n\n    # Zero gradients\n    encoder_optimizer.zero_grad()\n    decoder_optimizer.zero_grad()\n\n    #Set device options\n    input_variable = input_variable.to(device)\n    lengths = lengths.to(device)\n    target_variable = target_variable.to(device)\n    mask = mask.to(device)\n\n    # Initialize variables\n    loss = 0\n    print_losses = []\n    n_totals = 0\n\n    # Forward pass through encoder\n    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n\n    # Create initial decoder input (start with SOS tokens for each sentence)\n    decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])\n    decoder_input = decoder_input.to(device)\n\n    # Set initial decoder hidden state to the encoder's final hidden state\n    decoder_hidden = encoder_hidden[:decoder.n_layers]\n\n    # Determine if we are using teacher forcing this iteration\n    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n\n    # Forward batch of sequences through decoder one time step at a time\n    if use_teacher_forcing:\n        for t in range(max_target_len):\n            decoder_output, decoder_hidden = decoder(\n                decoder_input, decoder_hidden, encoder_outputs\n            )\n            # Teacher forcing: next input is current target\n            decoder_input = target_variable[t].view(1, -1)\n            # Calculate and accumulate loss\n            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n            loss += mask_loss\n            print_losses.append(mask_loss.item() * nTotal)\n            n_totals += nTotal\n    else:\n        for t in range(max_target_len):\n            decoder_output, decoder_hidden = decoder(\n                decoder_input, decoder_hidden, encoder_outputs\n            )\n            # No teacher forcing: next input is decoder's own current output\n            _, topi = decoder_output.topk(1)\n            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n            decoder_input = decoder_input.to(device)\n            # Calculate and accumulate loss\n            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n            loss += mask_loss\n            print_losses.append(mask_loss.item() * nTotal)\n            n_totals += nTotal\n\n    # Perform backpropatation\n    loss.backward()\n\n    # Clip gradients: gradients are modified in place\n    _ = nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n    _ = nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n\n    # Adjust model weights\n    encoder_optimizer.step()\n    decoder_optimizer.step()\n\n    return sum(print_losses) / n_totals","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer, embedding,\n               encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size, print_every, \n               save_every, clip, corpus_name, loadFilename):\n\n    # Load batches for each iteration\n    training_batches = [batch2TrainData(voc, [random.choice(pairs) for _ in range(batch_size)])\n                      for _ in range(n_iteration)]\n    \n    # Initializations\n    print('Initializing ...')\n    start_iteration = 1\n    print_loss = 0\n    if loadFilename:\n        start_iteration = checkpoint['iteration'] + 1\n\n    # Training loop\n    print(\"Training...\")\n    for iteration in range(start_iteration, n_iteration + 1):\n        training_batch = training_batches[iteration - 1]\n        # Extract fields from batch\n        input_variable, lengths, target_variable, mask, max_target_len = training_batch\n\n        # Run a training iteration with batch\n        loss = train(input_variable, lengths, target_variable, mask, max_target_len, encoder,\n                     decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip)\n        print_loss += loss\n\n        # Print progress\n        if iteration % print_every == 0:\n            print_loss_avg = print_loss / print_every\n            print(\"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\".format(iteration, iteration / n_iteration * 100, print_loss_avg))\n            print_loss = 0\n\n        # Save checkpoint\n        if (iteration % save_every == 0):\n            directory = os.path.join(outdirname, model_name, corpus_name, '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size))\n            if not os.path.exists(directory):\n                os.makedirs(directory)\n            torch.save({\n                'iteration': iteration,\n                'en': encoder.state_dict(),\n                'de': decoder.state_dict(),\n                'en_opt': encoder_optimizer.state_dict(),\n                'de_opt': decoder_optimizer.state_dict(),\n                'loss': loss,\n                'voc_dict': voc.__dict__,\n                'embedding': embedding.state_dict()\n            }, os.path.join(directory, '{}_{}.tar'.format(iteration, 'checkpoint')))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class GreedySearchDecoder(nn.Module):\n    def __init__(self, encoder, decoder):\n        super(GreedySearchDecoder, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, input_seq, input_length, max_length):\n        # Forward input through encoder model\n        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n        # Prepare encoder's final hidden layer to be first hidden input to the decoder\n        decoder_hidden = encoder_hidden[:decoder.n_layers]\n        # Initialize decoder input with SOS_token\n        decoder_input = torch.ones(1, 1,device=device, dtype=torch.long) * SOS_token\n        # Initialize tensors to append decoded words to\n        all_tokens = torch.zeros([0],device=device, dtype=torch.long)\n        all_scores = torch.zeros([0],device=device)\n        # Iteratively decode one word token at a time\n        for _ in range(max_length):\n            # Forward pass through decoder\n            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n            # Obtain most likely word token and its softmax score\n            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n            # Record token and score\n            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n            # Prepare current token to be next decoder input (add a dimension)\n            decoder_input = torch.unsqueeze(decoder_input, 0)\n        # Return collections of word tokens and scores\n        return all_tokens, all_scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate(encoder, decoder, searcher, voc, sentence, max_length=MAX_LENGTH):\n    ### Format input sentence as a batch\n    # words -> indexes\n    indexes_batch = [indexesFromSentence(voc, sentence)]\n    # Create lengths tensor\n    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n    # Transpose dimensions of batch to match models' expectations\n    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n    # Use appropriate device\n    input_batch = input_batch.to(device)\n    lengths = lengths.to(device)\n    # Decode sentence with searcher\n    tokens, scores = searcher(input_batch, lengths, max_length)\n    # indexes -> words\n    decoded_words = [voc.index2word[token.item()] for token in tokens]\n    return decoded_words\n\n\ndef evaluateInput(encoder, decoder, searcher, voc):\n    input_sentence = ''\n    while(1):\n        try:\n            # Get input sentence\n            input_sentence = input('> ')\n            # Check if it is quit case\n            if input_sentence == 'q' or input_sentence == 'quit': break\n            # Normalize sentence\n            input_sentence = normalizeString(input_sentence)\n            # Evaluate sentence\n            output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n            # Format and print response sentence\n            output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n            print('Bot:', ' '.join(output_words))\n\n        except KeyError:\n            print(\"Error: Encountered unknown word.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Run Model**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Configure models\nmodel_name = 'cb_model'\n#attn_model = 'dot'\nattn_model = 'general'\n#attn_model = 'concat'\nhidden_size = 500\nencoder_n_layers = 3\ndecoder_n_layers = 3\ndropout = 0.1\nbatch_size = 64\n\n# Set checkpoint to load from; set to None if starting from scratch\nloadFilename = None\ncheckpoint_iter = 4000\n#loadFilename = os.path.join(outdirname, model_name, corpus_name,\n                          # '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size),\n                           # '{}_checkpoint.tar'.format(4000))\n\n#loadFilename = '/kaggle/input/chatbotdataset/2500_checkpoint.tar'\n# Load model if a loadFilename is provided\nif loadFilename:\n    # If loading on same machine the model was trained on\n    checkpoint = torch.load(loadFilename)\n    # If loading a model trained on GPU to CPU\n    #checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n    encoder_sd = checkpoint['en']\n    decoder_sd = checkpoint['de']\n    encoder_optimizer_sd = checkpoint['en_opt']\n    decoder_optimizer_sd = checkpoint['de_opt']\n    embedding_sd = checkpoint['embedding']\n    voc.__dict__ = checkpoint['voc_dict']\n\n\nprint('Building encoder and decoder ...')\n# Initialize word embeddings\nembedding = nn.Embedding(voc.num_words, hidden_size)\nif loadFilename:\n    embedding.load_state_dict(embedding_sd)\n# Initialize encoder & decoder models\nencoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\ndecoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout)\nif loadFilename:\n    encoder.load_state_dict(encoder_sd)\n    decoder.load_state_dict(decoder_sd)\n# Use appropriate device\nencoder = encoder.to(device)\ndecoder = decoder.to(device)\nprint('Models built and ready to go!')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Run Training**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Configure training/optimization\nclip = 50.0\nteacher_forcing_ratio = 1.0\nlearning_rate = 0.0001\ndecoder_learning_ratio = 5.0\nn_iteration = 4000\nprint_every = 1\nsave_every = 500\n\n# Ensure dropout layers are in train mode\nencoder.train()\ndecoder.train()\n\n# Initialize optimizers\nprint('Building optimizers ...')\nencoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\ndecoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\nif loadFilename:\n    encoder_optimizer.load_state_dict(encoder_optimizer_sd)\n    decoder_optimizer.load_state_dict(decoder_optimizer_sd)\n\n# If you have cuda, configure cuda to call\nfor state in encoder_optimizer.state.values():\n    for k, v in state.items():\n        if isinstance(v, torch.Tensor):\n            state[k] = v.cuda()\n\nfor state in decoder_optimizer.state.values():\n    for k, v in state.items():\n        if isinstance(v, torch.Tensor):\n            state[k] = v.cuda()\n\n# Run training iterations\nprint(\"Starting Training!\")\ntrainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\n           embedding, encoder_n_layers, decoder_n_layers, outdirname, n_iteration, batch_size,\n           print_every, save_every, clip, corpus_name, loadFilename)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Run Evaluation*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set dropout layers to eval mode\nencoder.eval()\ndecoder.eval()\n\n# Initialize search module\nsearcher = GreedySearchDecoder(encoder, decoder)\n\n# Begin chatting (uncomment and run the following line to begin)\nevaluateInput(encoder, decoder, searcher, voc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"END\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}