{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\Documents\\aML\\chatbot\\4000_checkpoint.tar\n",
      "C:\\Users\\Lenovo\\Documents\\aML\\chatbot\\chat-bot.ipynb\n",
      "C:\\Users\\Lenovo\\Documents\\aML\\chatbot\\cornell_movie_dialogs_corpus.zip\n",
      "C:\\Users\\Lenovo\\Documents\\aML\\chatbot\\movie_conversations.txt\n",
      "C:\\Users\\Lenovo\\Documents\\aML\\chatbot\\movie_lines.txt\n",
      "C:\\Users\\Lenovo\\Documents\\aML\\chatbot\\.ipynb_checkpoints\\chat-bot-checkpoint.ipynb\n",
      "C:\\Users\\Lenovo\\Documents\\aML\\chatbot\\cornell movie-dialogs corpus\\.DS_Store\n",
      "C:\\Users\\Lenovo\\Documents\\aML\\chatbot\\cornell movie-dialogs corpus\\chameleons.pdf\n",
      "C:\\Users\\Lenovo\\Documents\\aML\\chatbot\\cornell movie-dialogs corpus\\formatted_movie_lines.txt\n",
      "C:\\Users\\Lenovo\\Documents\\aML\\chatbot\\cornell movie-dialogs corpus\\movie_characters_metadata.txt\n",
      "C:\\Users\\Lenovo\\Documents\\aML\\chatbot\\cornell movie-dialogs corpus\\movie_conversations.txt\n",
      "C:\\Users\\Lenovo\\Documents\\aML\\chatbot\\cornell movie-dialogs corpus\\movie_lines.txt\n",
      "C:\\Users\\Lenovo\\Documents\\aML\\chatbot\\cornell movie-dialogs corpus\\movie_titles_metadata.txt\n",
      "C:\\Users\\Lenovo\\Documents\\aML\\chatbot\\cornell movie-dialogs corpus\\raw_script_urls.txt\n",
      "C:\\Users\\Lenovo\\Documents\\aML\\chatbot\\cornell movie-dialogs corpus\\README.txt\n",
      "C:\\Users\\Lenovo\\Documents\\aML\\chatbot\\__MACOSX\\cornell movie-dialogs corpus\\._.DS_Store\n",
      "C:\\Users\\Lenovo\\Documents\\aML\\chatbot\\__MACOSX\\cornell movie-dialogs corpus\\._chameleons.pdf\n",
      "C:\\Users\\Lenovo\\Documents\\aML\\chatbot\\__MACOSX\\cornell movie-dialogs corpus\\._README.txt\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('C:\\\\Users\\\\Lenovo\\\\Documents\\\\aML\\\\chatbot'):\n",
    "   \n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # **IMPORTING LIBARARIES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "import codecs\n",
    "import itertools\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > # **Load & Preprocess Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name=\"C:\\\\Users\\\\Lenovo\\\\Documents\\\\aML\\\\chatbot\"\n",
    "lines_filepath = os.path.join(dir_name,'movie_lines.txt')\n",
    "conv_filepath = os.path.join(dir_name,'movie_conversations.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!\n",
      "L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!\n",
      "L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so.\n",
      "L984 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ She okay?\n",
      "L925 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Let's go.\n",
      "L924 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ Wow\n",
      "L872 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Okay -- you're gonna need to learn how to lie.\n",
      "L871 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ No\n"
     ]
    }
   ],
   "source": [
    "#visualize some movie lines...\n",
    "corpus_name = \"cornell movie-dialogs corpus\"\n",
    "with open(lines_filepath,'r',encoding='iso-8859-1') as file:\n",
    "    lines = file.readlines()\n",
    "for line in lines[:8]:\n",
    "    print(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split each line of the file into Dictionary of fields(lineID,characterID,movieID,character,text)\n",
    "line_fields = [\"lineID\",\"characterID\",\"movieID\",\"character\",\"text\"]\n",
    "lines = {}\n",
    "with open(lines_filepath, 'r', encoding='iso-8859-1') as f:\n",
    "    for line in f:\n",
    "        values = line.split(' +++$+++ ')\n",
    "        #Extract Fields\n",
    "        lineObj = {}\n",
    "        for i,field in enumerate(line_fields):\n",
    "            lineObj[field] = values[i]\n",
    "        lines[lineObj['lineID']] = lineObj    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'lineID': 'L1042',\n",
       "  'characterID': 'u5',\n",
       "  'movieID': 'm0',\n",
       "  'character': 'KAT',\n",
       "  'text': 'Oh, Bianca?  Can you get me my freshman yearbook?\\n'},\n",
       " {'lineID': 'L1043',\n",
       "  'characterID': 'u9',\n",
       "  'movieID': 'm0',\n",
       "  'character': 'PATRICK',\n",
       "  'text': \"Don ' t you even dare. . .\\n\"})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines['L1042'],lines['L1043']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groups fields of lines from 'Loadlines' into conversations based on \"movie_conversation.txt\"\n",
    "conv_fields = [\"character1ID\",\"character2ID\",\"movieID\",\"utteranceIDs\"]\n",
    "conversations = []\n",
    "with open(conv_filepath, 'r', encoding='iso-8859-1') as f:\n",
    "    for line in f:\n",
    "        values = line.split(\" +++$+++ \")\n",
    "        #Extract fields\n",
    "        convObj = {}\n",
    "       \n",
    "        for i,field in enumerate(conv_fields):\n",
    "            convObj[field] = values[i]\n",
    "        #convert string results from split to list. convObj['utteranceIDs'] == ['L1','L2',...]\n",
    "            \n",
    "        lineIds = eval(convObj[\"utteranceIDs\"])\n",
    "        #reassemble lines\n",
    "        convObj['lines'] = []\n",
    "        for lineId in lineIds:\n",
    "            convObj['lines'].append(lines[lineId]) \n",
    "        conversations.append(convObj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'character1ID': 'u0',\n",
       " 'character2ID': 'u2',\n",
       " 'movieID': 'm0',\n",
       " 'utteranceIDs': \"['L194', 'L195', 'L196', 'L197']\\n\",\n",
       " 'lines': [{'lineID': 'L194',\n",
       "   'characterID': 'u0',\n",
       "   'movieID': 'm0',\n",
       "   'character': 'BIANCA',\n",
       "   'text': 'Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\\n'},\n",
       "  {'lineID': 'L195',\n",
       "   'characterID': 'u2',\n",
       "   'movieID': 'm0',\n",
       "   'character': 'CAMERON',\n",
       "   'text': \"Well, I thought we'd start with pronunciation, if that's okay with you.\\n\"},\n",
       "  {'lineID': 'L196',\n",
       "   'characterID': 'u0',\n",
       "   'movieID': 'm0',\n",
       "   'character': 'BIANCA',\n",
       "   'text': 'Not the hacking and gagging and spitting part.  Please.\\n'},\n",
       "  {'lineID': 'L197',\n",
       "   'characterID': 'u2',\n",
       "   'movieID': 'm0',\n",
       "   'character': 'CAMERON',\n",
       "   'text': \"Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\\n\"}]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract pair of sentences from conversations\n",
    "qa_pairs = []\n",
    "for conversation in conversations:\n",
    "    for i in range(len(conversation['lines'])-1):\n",
    "        imputline = conversation['lines'][i]['text'].strip()\n",
    "        targetline = conversation['lines'][i+1]['text'].strip()\n",
    "    \n",
    "        if imputline and targetline:\n",
    "            qa_pairs.append([imputline,targetline])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "221282"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Not the hacking and gagging and spitting part.  Please.',\n",
       " \"Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\"]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_pairs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "writing newly formatted file...\n",
      "Done writing to file...\n"
     ]
    }
   ],
   "source": [
    "#define path to new file\n",
    "outdirname= dir_name\n",
    "datafile = os.path.join(outdirname,\"formatted_movie_lines.txt\")\n",
    "delimiter = '\\t'\n",
    "delimiter = str(codecs.decode(delimiter,'unicode_escape'))\n",
    "\n",
    "#write new csv file\n",
    "print(\"\\nwriting newly formatted file...\")\n",
    "with open(datafile,'w',encoding = 'utf-8') as outputfile:\n",
    "    writer = csv.writer(outputfile,delimiter=delimiter)\n",
    "    for pair in qa_pairs:\n",
    "        writer.writerow(pair)\n",
    "print(\"Done writing to file...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\\tWell, I thought we'd start with pronunciation, if that's okay with you.\\r\\r\\n\"\n",
      "b\"Well, I thought we'd start with pronunciation, if that's okay with you.\\tNot the hacking and gagging and spitting part.  Please.\\r\\r\\n\"\n",
      "b\"Not the hacking and gagging and spitting part.  Please.\\tOkay... then how 'bout we try out some French cuisine.  Saturday?  Night?\\r\\r\\n\"\n",
      "b\"You're asking me out.  That's so cute. What's your name again?\\tForget it.\\r\\r\\n\"\n",
      "b\"No, no, it's my fault -- we didn't have a proper introduction ---\\tCameron.\\r\\r\\n\"\n",
      "b\"Cameron.\\tThe thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\\r\\r\\n\"\n",
      "b\"The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\\tSeems like she could get a date easy enough...\\r\\r\\n\"\n",
      "b'Why?\\tUnsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.\\r\\r\\n'\n"
     ]
    }
   ],
   "source": [
    "#visualize some lines\n",
    "datafile = os.path.join(outdirname,\"formatted_movie_lines.txt\")\n",
    "with open(datafile,'rb') as file:\n",
    "    lines = file.readlines()\n",
    "for line in lines[:8]:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making the vocabulary\n",
    "PAD_token = 0  #for padding short senetnces\n",
    "SOS_token = 1  #start of sentence token<START>\n",
    "EOS_token = 2  #end of sentence token<END>\n",
    "\n",
    "class vocabulary:\n",
    "    def __init__(self,name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: 'PAD',SOS_token:'SOS',EOS_token:'EOS'}\n",
    "        self.num_words = 3 # count PAD,SOS,EOS\n",
    "        \n",
    "    def addSentence(self,sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "    \n",
    "    def addWord(self,word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words +=1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "    \n",
    "    #remove words below a threshhold\n",
    "    def trim(self, min_count):\n",
    "        keep_words = []\n",
    "        for k,v in self.word2count.items():\n",
    "            if v >= min_count:\n",
    "                keep_words.append(k)\n",
    "        \n",
    "        print('keep_words {} / {} = {:.4f}'.format(\n",
    "            len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)\n",
    "        ))\n",
    "        \n",
    "        # Reinitialize dictionaries\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3 # Count default tokens\n",
    "\n",
    "        for word in keep_words:\n",
    "            self.addWord(word)\n",
    "            \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn a unicode string to plain ASCII\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD',s) if unicodedata.category(c) != 'Mn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a A e E i I o'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test the function\n",
    "unicodeToAscii(\"á Á é É í Í ó\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    #Replace any .!? by a whitespace + that character ('!' by ' !'). \\1 means the first bracketed group[.!?].\n",
    "    # r is to not consider \\1 as escape character.\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    # Remove any character that is nota sequence of lower and upper case letters. + means one or more.\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    #remove sequence of whitespace characters.\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aaa aa !s s . dd ?'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the function\n",
    "normalizeString(\"aaa123aa!s's . dd?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Readaing and processing file... please wait.\n",
      "Done reading...\n"
     ]
    }
   ],
   "source": [
    "datafile = os.path.join(outdirname, \"formatted_movie_lines.txt\")\n",
    "# readthe file and split into lines\n",
    "print(\"Readaing and processing file... please wait.\")\n",
    "lines = open(datafile, encoding = 'utf-8').read().strip().split('\\n')\n",
    "#split every line into pairs and normalize.\n",
    "pairs =[[normalizeString(s) for s in pair.split(\"\\t\")] for pair in lines]\n",
    "print(\"Done reading...\")\n",
    "voc = vocabulary(\"cornell movie-dialogs corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['can we make this quick ? roxanne korrine and andrew barrett are having an incredibly horrendous public break up on the quad . again .',\n",
       " 'well i thought we d start with pronunciation if that s okay with you .']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns True if both the sentences in a pair 'p' are under the MAX_LENGTH Threshhold.\n",
    "MAX_LENGTH = 10\n",
    "def filterPair(p):\n",
    "    #input sequences must preserve the last word for EOS token\n",
    "    return len(p[0].split()) < MAX_LENGTH and len(p[1].split()) < MAX_LENGTH\n",
    "#Filter pairs using filter pair conditions.\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 221282 pairs/conversations\n",
      "There are 64271 pairs/conversations\n"
     ]
    }
   ],
   "source": [
    "pairs =[pair for pair in pairs if len(pair)>1]\n",
    "print(\"There are {} pairs/conversations\".format(len(pairs)))\n",
    "pairs = filterPairs(pairs)\n",
    "print(\"There are {} pairs/conversations\".format(len(pairs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 18008\n",
      "['there .', 'where ?']\n",
      "['you have my word . as a gentleman', 'you re sweet .']\n",
      "['hi .', 'looks like things worked out tonight huh ?']\n",
      "['you know chastity ?', 'i believe we share an art instructor']\n",
      "['have fun tonight ?', 'tons']\n",
      "['well no . . .', 'then that s all you had to say .']\n",
      "['then that s all you had to say .', 'but']\n",
      "['but', 'you always been this selfish ?']\n",
      "['do you listen to this crap ?', 'what crap ?']\n",
      "['what good stuff ?', 'the real you .']\n"
     ]
    }
   ],
   "source": [
    "#loop through each pair and add question and reply sequence to vocabulary\n",
    "for pair in pairs:\n",
    "    voc.addSentence(pair[0])\n",
    "    voc.addSentence(pair[1])\n",
    "print(\"Number of words:\",voc.num_words)\n",
    "for pair in pairs[:10]:\n",
    "    print(pair)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keep_words 7823 / 18005 = 0.4345\n",
      "Trimmed from 64271 pairs to 53165, 0.8272 of total\n"
     ]
    }
   ],
   "source": [
    "MIN_COUNT = 3    # Minimum word count threshold for trimming\n",
    "\n",
    "def trimRareWords(voc, pairs, MIN_COUNT):\n",
    "    # Trim words used under the MIN_COUNT from the voc\n",
    "    voc.trim(MIN_COUNT)\n",
    "    # Filter out pairs with trimmed words\n",
    "    keep_pairs = []\n",
    "    for pair in pairs:\n",
    "        input_sentence = pair[0]\n",
    "        output_sentence = pair[1]\n",
    "        keep_input = True\n",
    "        keep_output = True\n",
    "        # Check input sentence\n",
    "        for word in input_sentence.split(' '):\n",
    "            if word not in voc.word2index:\n",
    "                keep_input = False\n",
    "                break\n",
    "        # Check output sentence\n",
    "        for word in output_sentence.split(' '):\n",
    "            if word not in voc.word2index:\n",
    "                keep_output = False\n",
    "                break\n",
    "\n",
    "        # Only keep pairs that do not contain trimmed word(s) in their input or output sentence\n",
    "        if keep_input and keep_output:\n",
    "            keep_pairs.append(pair)\n",
    "\n",
    "    print(\"Trimmed from {} pairs to {}, {:.4f} of total\".format(len(pairs), len(keep_pairs), len(keep_pairs) / len(pairs)))\n",
    "    return keep_pairs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Trim voc and pairs\n",
    "pairs = trimRareWords(voc, pairs, MIN_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variable: tensor([[  25,    5,    7,   34,  240],\n",
      "        [  89,  148,   14,   61,   56],\n",
      "        [  44, 3232,  459,  331,  945],\n",
      "        [1633,  746,   56,  117,    6],\n",
      "        [  75,    7,  174,    4,    2],\n",
      "        [  45,    6,    4,    2,    0],\n",
      "        [   4,    2,    2,    0,    0],\n",
      "        [   2,    0,    0,    0,    0]])\n",
      "lengths: tensor([8, 7, 7, 6, 5])\n",
      "target_variable: tensor([[ 334,  101,   77,  100,   56],\n",
      "        [5879,  315,  271,   80, 4087],\n",
      "        [ 289,    9,  117,    7,    4],\n",
      "        [  75, 1160,    6,   59,    2],\n",
      "        [  83,   77,    2,  170,    0],\n",
      "        [   6,  969,    0,   40,    0],\n",
      "        [   2,    4,    0,   47,    0],\n",
      "        [   0,    2,    0,   36,    0],\n",
      "        [   0,    0,    0,    6,    0],\n",
      "        [   0,    0,    0,    2,    0]])\n",
      "mask: tensor([[ True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False],\n",
      "        [ True,  True, False,  True, False],\n",
      "        [ True,  True, False,  True, False],\n",
      "        [False,  True, False,  True, False],\n",
      "        [False, False, False,  True, False],\n",
      "        [False, False, False,  True, False]])\n",
      "max_target_len: 10\n"
     ]
    }
   ],
   "source": [
    "def indexesFromSentence(voc, sentence):\n",
    "    return [voc.word2index[word] for word in sentence.split(' ')] + [EOS_token]\n",
    "\n",
    "# It zips each entries with corresponding entries in other list and pads 0 where there is no entry,\n",
    "# in this process the original matrix is transposed automatically. \n",
    "def zeroPadding(l, fillvalue=PAD_token): #pads the matrix with fillvalue(which is 0 here) where it is short, to make it m x n.\n",
    "    return list(itertools.zip_longest(*l, fillvalue=fillvalue)) #where m is length of longest question which is 10 here.\n",
    "\n",
    "def binaryMatrix(l, value=PAD_token): # creates binary matrix, if there is a value, 1 is stored else 0 for no value.\n",
    "    m = []\n",
    "    for i, seq in enumerate(l):\n",
    "        m.append([])                #for every list of values there is a list \n",
    "        for token in seq:\n",
    "            if token == PAD_token:\n",
    "                m[i].append(0)\n",
    "            else:\n",
    "                m[i].append(1)\n",
    "    return m\n",
    "\n",
    "# Returns padded input sequence tensor and lengths\n",
    "def inputVar(l, voc):\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, lengths\n",
    "\n",
    "# Returns padded target sequence tensor, padding mask, and max target length\n",
    "def outputVar(l, voc):\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
    "    max_target_len = max([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    mask = binaryMatrix(padList)\n",
    "    mask = torch.BoolTensor(mask)\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, mask, max_target_len\n",
    "\n",
    "# Returns all items for a given batch of pairs\n",
    "#1 sorting batch with key as length means the qa_pair with longer question length is before all other pair in the list.\n",
    "def batch2TrainData(voc, pair_batch):\n",
    "    pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True) #1\n",
    "    input_batch, output_batch = [], []\n",
    "    for pair in pair_batch:\n",
    "        input_batch.append(pair[0])\n",
    "        output_batch.append(pair[1])\n",
    "    inp, lengths = inputVar(input_batch, voc)\n",
    "    output, mask, max_target_len = outputVar(output_batch, voc)\n",
    "    return inp, lengths, output, mask, max_target_len\n",
    "\n",
    "\n",
    "# Example for validation\n",
    "small_batch_size = 5\n",
    "batches = batch2TrainData(voc, [random.choice(pairs) for _ in range(small_batch_size)])\n",
    "input_variable, lengths, target_variable, mask, max_target_len = batches\n",
    "\n",
    "print(\"input_variable:\", input_variable) # input transposed when matrix was padded above.\n",
    "print(\"lengths:\", lengths) # in desc order bcoz of sorting with key as length of question\n",
    "print(\"target_variable:\", target_variable) # answer corresponds to each question(column) in the input matrix\n",
    "print(\"mask:\", mask)\n",
    "print(\"max_target_len:\", max_target_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Seq2Seq Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  ### Encoder "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inputs:\n",
    "\n",
    "    input_seq: batch of input sentences; shape=(max_length, batch_size)...\n",
    "   \n",
    "    input_lengths: list of sentence lengths corresponding to each sentence in the batch; shape=(batch_size)...\n",
    "   \n",
    "    hidden: hidden state; shape=(n_layers x num_directions, batch_size, hidden_size)\n",
    "\n",
    "Outputs:\n",
    "\n",
    "    outputs: output features from the last hidden layer of the GRU (sum of bidirectional outputs); shape=(max_length, batch_size, hidden_size)...\n",
    "   \n",
    "    hidden: updated hidden state from GRU; shape=(n_layers x num_directions, batch_size, hidden_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = embedding\n",
    "\n",
    "        # Initialize GRU; the input_size and hidden_size params are both set to 'hidden_size'\n",
    "        #   because our input size is a word embedding with number of features == hidden_size\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,\n",
    "                          dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\n",
    "\n",
    "    def forward(self, input_seq, input_lengths, hidden=None):\n",
    "        # Convert word indexes to embeddings\n",
    "        embedded = self.embedding(input_seq)\n",
    "        # Pack padded batch of sequences for RNN module\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
    "        # Forward pass through GRU\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        # Unpack padding\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        # Sum bidirectional GRU outputs\n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n",
    "        # Return output and final hidden state\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> attention model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Luong attention layer\n",
    "class Attn(nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.method = method\n",
    "        if self.method not in ['dot', 'general', 'concat']:\n",
    "            raise ValueError(self.method, \"is not an appropriate attention method.\")\n",
    "        self.hidden_size = hidden_size\n",
    "        if self.method == 'general':\n",
    "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "            self.v = nn.Parameter(torch.FloatTensor(hidden_size))\n",
    "\n",
    "    def dot_score(self, hidden, encoder_output):\n",
    "        #Element wise multiply the current decoder state with the encoder output and sum them along dim=2.\n",
    "        return torch.sum(hidden * encoder_output, dim=2)\n",
    "\n",
    "    def general_score(self, hidden, encoder_output):\n",
    "        energy = self.attn(encoder_output)\n",
    "        return torch.sum(hidden * energy, dim=2)\n",
    "\n",
    "    def concat_score(self, hidden, encoder_output):\n",
    "        energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()\n",
    "        return torch.sum(self.v * energy, dim=2)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # hidden of shape: (1, batch_size, hidden_size)\n",
    "        #encoder_outputs of shape: (max_length, batch_size, hidden_size)\n",
    "        \n",
    "        # Calculate the attention weights (energies) based on the given method\n",
    "        if self.method == 'general':\n",
    "            attn_energies = self.general_score(hidden, encoder_outputs)\n",
    "        elif self.method == 'concat':\n",
    "            attn_energies = self.concat_score(hidden, encoder_outputs)\n",
    "        elif self.method == 'dot':\n",
    "            attn_energies = self.dot_score(hidden, encoder_outputs) # (max_length,batch_size),after summing along hiddden size(dim=2)\n",
    "\n",
    "        # Transpose max_length and batch_size dimensions\n",
    "        attn_energies = attn_energies.t()                           # (batch_size, max_length)\n",
    "\n",
    "        # Return the softmax normalized probability scores (with added dimension)\n",
    "        return F.softmax(attn_energies, dim=1).unsqueeze(1)    # (batch_size, 1, max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Luong Attention Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inputs:\n",
    "\n",
    "    input_step: one time step (one word) of input sequence batch; shape=(1, batch_size)\n",
    "    last_hidden: final hidden layer of GRU; shape=(n_layers x num_directions, batch_size, hidden_size)\n",
    "    encoder_outputs: encoder model’s output; shape=(max_length, batch_size, hidden_size)\n",
    "\n",
    "Outputs:\n",
    "\n",
    "    output: softmax normalized tensor giving probabilities of each word being the correct next word in the decoded sequence; shape=(batch_size, voc.num_words)\n",
    "    hidden: final hidden state of GRU; shape=(n_layers x num_directions, batch_size, hidden_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
    "        super(LuongAttnDecoderRNN, self).__init__()\n",
    "\n",
    "        # Keep for reference\n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Define layers\n",
    "        self.embedding = embedding\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        self.attn = Attn(attn_model, hidden_size)\n",
    "\n",
    "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
    "        #input_step: one time step (1 word) of input sequence batch, shape = (1,batch_size)\n",
    "        #last_hidden: final hidden state of encoder's GRU; shape = (n_layers x num_direcions, batch_size, hidden_size)\n",
    "        #encoder_outputs: encoder model's output; shape = (max_length, batch_size, hidden_size)\n",
    "        \n",
    "        # Note: we run this one step (word) at a time\n",
    "        # Get embedding of current input word\n",
    "        embedded = self.embedding(input_step)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        # Forward through unidirectional GRU\n",
    "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
    "        #rnn_output of shape: (1,batch,num_directions x hidden_size)\n",
    "        #last_hidden of shape: (n_layers x num_direcions, batch_size, hidden_size)\n",
    "        \n",
    "        # Calculate attention weights from the current GRU output\n",
    "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
    "        # Multiply attention weights to encoder outputs to get new \"weighted sum\" context vector\n",
    "        #(batch_size, 1, max_length) bmm with (batch_size, max_length, hidden) = (batch_size, 1, hidden)\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
    "        # Concatenate weighted context vector and GRU output using Luong eq. 5\n",
    "        rnn_output = rnn_output.squeeze(0)\n",
    "        context = context.squeeze(1)\n",
    "        concat_input = torch.cat((rnn_output, context), 1)\n",
    "        concat_output = torch.tanh(self.concat(concat_input))\n",
    "        # Predict next word using Luong eq. 6\n",
    "        output = self.out(concat_output)\n",
    "        output = F.softmax(output, dim=1)\n",
    "        # Return output and final hidden state\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Traning the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Masked LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This loss function calculates the average negative log likelihood of the elements\n",
    "#that correspond to a 1 in the mask tensor.\n",
    "\n",
    "def maskNLLLoss(inp, target, mask):\n",
    "    nTotal = mask.sum()\n",
    "    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n",
    "    loss = crossEntropy.masked_select(mask).mean()\n",
    "    return loss, nTotal.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6065, 0.0988, 0.5122, 0.9869, 0.9087, 0.6228, 0.1591],\n",
      "        [0.3654, 0.1991, 0.3438, 0.6460, 0.7563, 0.5935, 0.4577],\n",
      "        [0.9402, 0.7217, 0.0872, 0.0887, 0.9356, 0.6957, 0.4502],\n",
      "        [0.2010, 0.4043, 0.2351, 0.3570, 0.0830, 0.2276, 0.0629],\n",
      "        [0.2761, 0.3901, 0.7730, 0.1016, 0.5399, 0.9729, 0.5490]])\n",
      "tensor([[0.1432, 0.0862, 0.1303, 0.2095, 0.1937, 0.1455, 0.0915],\n",
      "        [0.1253, 0.1061, 0.1226, 0.1659, 0.1852, 0.1574, 0.1374],\n",
      "        [0.1981, 0.1592, 0.0844, 0.0845, 0.1972, 0.1551, 0.1214],\n",
      "        [0.1386, 0.1698, 0.1434, 0.1620, 0.1232, 0.1423, 0.1207],\n",
      "        [0.1084, 0.1215, 0.1781, 0.0910, 0.1411, 0.2175, 0.1424]])\n",
      "tensor([[2],\n",
      "        [1],\n",
      "        [5],\n",
      "        [4],\n",
      "        [0]])\n",
      "tensor([[0.1303],\n",
      "        [0.1061],\n",
      "        [0.1551],\n",
      "        [0.1232],\n",
      "        [0.1084]])\n",
      "torch.Size([5, 1])\n",
      "Cross Entropy: tensor([[2.0378],\n",
      "        [2.2433],\n",
      "        [1.8635],\n",
      "        [2.0943],\n",
      "        [2.2222]])\n",
      "Loss : tensor([2.0378, 1.8635, 2.0943])\n",
      "torch.Size([3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\aten\\src\\ATen\\native\\LegacyDefinitions.cpp:29: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum of the mask element:  tensor(3)\n",
      "Mean of loss :  tensor(1.9985)\n",
      "Mean of cross entropy without mask...:  tensor(2.0922)\n"
     ]
    }
   ],
   "source": [
    "#visualisation of loss function\n",
    "#decoder_output shape:(batcH_size,vocab_size); target size = (batch_size,1)\n",
    "dec_o = torch.rand(5,7)\n",
    "print(dec_o)\n",
    "dec_o = F.softmax(dec_o,dim = 1)\n",
    "tar = torch.tensor([2,1,5,4,0],dtype = torch.long)\n",
    "tar = tar.view(-1,1)\n",
    "mask = torch.tensor([1,0,1,1,0],dtype= torch.uint8)\n",
    "print(dec_o)\n",
    "print(tar)\n",
    "gath_ten = torch.gather(dec_o,1,tar)\n",
    "print(gath_ten)\n",
    "print(gath_ten.shape)\n",
    "crossEntropy = -torch.log(gath_ten)\n",
    "print(\"Cross Entropy:\", crossEntropy)\n",
    "mask = mask.unsqueeze(1)\n",
    "loss = crossEntropy.masked_select(mask)\n",
    "print(\"Loss :\",loss)\n",
    "print(loss.shape)\n",
    "print(\"sum of the mask element: \",mask.sum())\n",
    "print(\"Mean of loss : \", loss.mean())\n",
    "print(\"Mean of cross entropy without mask...: \",crossEntropy.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding,\n",
    "          encoder_optimizer, decoder_optimizer, batch_size, clip, max_length=MAX_LENGTH):\n",
    "\n",
    "    # Zero gradients\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    # Set device options\n",
    "    #input_variable = input_variable.to(device)\n",
    "    #lengths = lengths.to(device)\n",
    "    #target_variable = target_variable.to(device)\n",
    "    #mask = mask.to(device)\n",
    "\n",
    "    # Initialize variables\n",
    "    loss = 0\n",
    "    print_losses = []\n",
    "    n_totals = 0\n",
    "\n",
    "    # Forward pass through encoder\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
    "\n",
    "    # Create initial decoder input (start with SOS tokens for each sentence)\n",
    "    decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])\n",
    "    #decoder_input = decoder_input.to(device)\n",
    "\n",
    "    # Set initial decoder hidden state to the encoder's final hidden state\n",
    "    decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "\n",
    "    # Determine if we are using teacher forcing this iteration\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    # Forward batch of sequences through decoder one time step at a time\n",
    "    if use_teacher_forcing:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            # Teacher forcing: next input is current target\n",
    "            decoder_input = target_variable[t].view(1, -1)\n",
    "            # Calculate and accumulate loss\n",
    "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "    else:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            # No teacher forcing: next input is decoder's own current output\n",
    "            _, topi = decoder_output.topk(1)\n",
    "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
    "            #decoder_input = decoder_input.to(device)\n",
    "            # Calculate and accumulate loss\n",
    "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "\n",
    "    # Perform backpropatation\n",
    "    loss.backward()\n",
    "\n",
    "    # Clip gradients: gradients are modified in place\n",
    "    _ = nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "    _ = nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "\n",
    "    # Adjust model weights\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return sum(print_losses) / n_totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer, embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size, print_every, save_every, clip, corpus_name, loadFilename):\n",
    "\n",
    "    # Load batches for each iteration\n",
    "    training_batches = [batch2TrainData(voc, [random.choice(pairs) for _ in range(batch_size)])\n",
    "                      for _ in range(n_iteration)]\n",
    "    \n",
    "    # Initializations\n",
    "    print('Initializing ...')\n",
    "    start_iteration = 1\n",
    "    print_loss = 0\n",
    "    if loadFilename:\n",
    "        start_iteration = checkpoint['iteration'] + 1\n",
    "\n",
    "    # Training loop\n",
    "    print(\"Training...\")\n",
    "    for iteration in range(start_iteration, n_iteration + 1):\n",
    "        training_batch = training_batches[iteration - 1]\n",
    "        # Extract fields from batch\n",
    "        input_variable, lengths, target_variable, mask, max_target_len = training_batch\n",
    "\n",
    "        # Run a training iteration with batch\n",
    "        loss = train(input_variable, lengths, target_variable, mask, max_target_len, encoder,\n",
    "                     decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip)\n",
    "        print_loss += loss\n",
    "\n",
    "        # Print progress\n",
    "        if iteration % print_every == 0:\n",
    "            print_loss_avg = print_loss / print_every\n",
    "            print(\"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\".format(iteration, iteration / n_iteration * 100, print_loss_avg))\n",
    "            print_loss = 0\n",
    "\n",
    "        # Save checkpoint\n",
    "        if (iteration % save_every == 0):\n",
    "            directory = os.path.join(outdirname, model_name, corpus_name, '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size))\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "            torch.save({\n",
    "                'iteration': iteration,\n",
    "                'en': encoder.state_dict(),\n",
    "                'de': decoder.state_dict(),\n",
    "                'en_opt': encoder_optimizer.state_dict(),\n",
    "                'de_opt': decoder_optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "                'voc_dict': voc.__dict__,\n",
    "                'embedding': embedding.state_dict()\n",
    "            }, os.path.join(directory, '{}_{}.tar'.format(iteration, 'checkpoint')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedySearchDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(GreedySearchDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, input_seq, input_length, max_length):\n",
    "        # Forward input through encoder model\n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
    "        # Prepare encoder's final hidden layer to be first hidden input to the decoder\n",
    "        decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "        # Initialize decoder input with SOS_token\n",
    "        decoder_input = torch.ones(1, 1, dtype=torch.long) * SOS_token\n",
    "        # Initialize tensors to append decoded words to\n",
    "        all_tokens = torch.zeros([0], dtype=torch.long)\n",
    "        all_scores = torch.zeros([0])\n",
    "        # Iteratively decode one word token at a time\n",
    "        for _ in range(max_length):\n",
    "            # Forward pass through decoder\n",
    "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            # Obtain most likely word token and its softmax score\n",
    "            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
    "            # Record token and score\n",
    "            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
    "            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
    "            # Prepare current token to be next decoder input (add a dimension)\n",
    "            decoder_input = torch.unsqueeze(decoder_input, 0)\n",
    "        # Return collections of word tokens and scores\n",
    "        return all_tokens, all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, searcher, voc, sentence, max_length=MAX_LENGTH):\n",
    "    ### Format input sentence as a batch\n",
    "    # words -> indexes\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence)]\n",
    "    # Create lengths tensor\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    # Transpose dimensions of batch to match models' expectations\n",
    "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
    "    # Use appropriate device\n",
    "    #input_batch = input_batch.to(device)\n",
    "    #lengths = lengths.to(device)\n",
    "    # Decode sentence with searcher\n",
    "    tokens, scores = searcher(input_batch, lengths, max_length)\n",
    "    # indexes -> words\n",
    "    decoded_words = [voc.index2word[token.item()] for token in tokens]\n",
    "    return decoded_words\n",
    "\n",
    "\n",
    "def evaluateInput(encoder, decoder, searcher, voc):\n",
    "    input_sentence = ''\n",
    "    while(1):\n",
    "        try:\n",
    "            # Get input sentence\n",
    "            input_sentence = input('> ')\n",
    "            # Check if it is quit case\n",
    "            if input_sentence == 'q' or input_sentence == 'quit': break\n",
    "            # Normalize sentence\n",
    "            input_sentence = normalizeString(input_sentence)\n",
    "            # Evaluate sentence\n",
    "            output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
    "            # Format and print response sentence\n",
    "            output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
    "            print('Bot:', ' '.join(output_words))\n",
    "\n",
    "        except KeyError:\n",
    "            print(\"Error: Encountered unknown word.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n"
     ]
    }
   ],
   "source": [
    "# Configure models\n",
    "model_name = 'cb_model'\n",
    "attn_model = 'dot'\n",
    "#attn_model = 'general'\n",
    "#attn_model = 'concat'\n",
    "hidden_size = 500\n",
    "encoder_n_layers = 2\n",
    "decoder_n_layers = 2\n",
    "dropout = 0.1\n",
    "batch_size = 64\n",
    "\n",
    "# Set checkpoint to load from; set to None if starting from scratch\n",
    "loadFilename = None\n",
    "checkpoint_iter = 4000\n",
    "#loadFilename = os.path.join(outdirname, model_name, corpus_name,\n",
    "                          # '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size),\n",
    "                           # '{}_checkpoint.tar'.format(4000))\n",
    "\n",
    "loadFilename = os.path.join(dir_name,'4000_checkpoint.tar')\n",
    "# Load model if a loadFilename is provided\n",
    "if loadFilename:\n",
    "    # If loading on same machine the model was trained on\n",
    "    checkpoint = torch.load(loadFilename)\n",
    "    # If loading a model trained on GPU to CPU\n",
    "    #checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n",
    "    encoder_sd = checkpoint['en']\n",
    "    decoder_sd = checkpoint['de']\n",
    "    encoder_optimizer_sd = checkpoint['en_opt']\n",
    "    decoder_optimizer_sd = checkpoint['de_opt']\n",
    "    embedding_sd = checkpoint['embedding']\n",
    "    voc.__dict__ = checkpoint['voc_dict']\n",
    "\n",
    "\n",
    "print('Building encoder and decoder ...')\n",
    "# Initialize word embeddings\n",
    "embedding = nn.Embedding(voc.num_words, hidden_size)\n",
    "if loadFilename:\n",
    "    embedding.load_state_dict(embedding_sd)\n",
    "# Initialize encoder & decoder models\n",
    "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
    "decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout)\n",
    "if loadFilename:\n",
    "    encoder.load_state_dict(encoder_sd)\n",
    "    decoder.load_state_dict(decoder_sd)\n",
    "# Use appropriate device\n",
    "#encoder = encoder.to(device)\n",
    "#decoder = decoder.to(device)\n",
    "print('Models built and ready to go!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building optimizers ...\n",
      "Starting Training!\n",
      "Initializing ...\n",
      "Training...\n"
     ]
    }
   ],
   "source": [
    "# Configure training/optimization\n",
    "clip = 50.0\n",
    "teacher_forcing_ratio = 1.0\n",
    "learning_rate = 0.0001\n",
    "decoder_learning_ratio = 5.0\n",
    "n_iteration = 4000\n",
    "print_every = 1\n",
    "save_every = 500\n",
    "\n",
    "# Ensure dropout layers are in train mode\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "\n",
    "# Initialize optimizers\n",
    "print('Building optimizers ...')\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
    "if loadFilename:\n",
    "    encoder_optimizer.load_state_dict(encoder_optimizer_sd)\n",
    "    decoder_optimizer.load_state_dict(decoder_optimizer_sd)\n",
    "\n",
    "# If you have cuda, configure cuda to call\n",
    "#for state in encoder_optimizer.state.values():\n",
    "    #for k, v in state.items():\n",
    "        #if isinstance(v, torch.Tensor):\n",
    "            #state[k] = v.cuda()\n",
    "\n",
    "#for state in decoder_optimizer.state.values():\n",
    "    #for k, v in state.items():\n",
    "        #if isinstance(v, torch.Tensor):\n",
    "            #state[k] = v.cuda()\n",
    "\n",
    "# Run training iterations\n",
    "print(\"Starting Training!\")\n",
    "trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
    "           embedding, encoder_n_layers, decoder_n_layers, outdirname, n_iteration, batch_size,\n",
    "           print_every, save_every, clip, corpus_name, loadFilename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Run Evaluation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> hey\n",
      "Bot: what ? ? ? ? ?\n",
      "> what is your name?\n",
      "Bot: victor frankenstein . is . . .\n",
      "> fuck\n",
      "Bot: i m sorry . is .\n",
      "> where are you?\n",
      "Bot: i m leaving . s in . .\n",
      "> shutup\n",
      "Error: Encountered unknown word.\n",
      "> shut it\n",
      "Bot: it s okay . is it ?\n",
      "> shut up\n",
      "Bot: i m sorry . is .\n",
      "> okay go now\n",
      "Bot: okay . is all right . .\n",
      "> q\n"
     ]
    }
   ],
   "source": [
    "# Set dropout layers to eval mode\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# Initialize search module\n",
    "searcher = GreedySearchDecoder(encoder, decoder)\n",
    "\n",
    "# Begin chatting (uncomment and run the following line to begin)\n",
    "evaluateInput(encoder, decoder, searcher, voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END\n"
     ]
    }
   ],
   "source": [
    "print(\"END\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
